{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install watermark"
      ],
      "metadata": {
        "id": "mNrTmrR0m4Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "JkysPFJum4La"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -a \"Rishit Kapoor 21BAI1046\" -u -d -v -m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOOvMycFm4A-",
        "outputId": "468b8026-748a-4413-d556-7ea965187f0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Rishit Kapoor 21BAI1046\n",
            "\n",
            "Last updated: 2024-04-13\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "Compiler    : GCC 11.4.0\n",
            "OS          : Linux\n",
            "Release     : 6.1.58+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "ucyDY2d6m3-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "rxByfxvkm38C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "pmmEBVYGm333"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "QwF0eX8Xm31D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download rohanrao/nifty50-stock-market-data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vlx7n2UnFfR",
        "outputId": "7c2c5140-42ca-4ca0-9f6e-7b05a0785e64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nifty50-stock-market-data.zip to /content\n",
            " 92% 17.0M/18.4M [00:01<00:00, 16.7MB/s]\n",
            "100% 18.4M/18.4M [00:01<00:00, 11.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/nifty50-stock-market-data.zip' -d '/content/nifty50-stock-market-data'"
      ],
      "metadata": {
        "id": "f_Ux1bqonFNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"/content/nifty50-stock-market-data/NIFTY50_all.csv\")\n",
        "\n",
        "# Select only the 'Close' price column\n",
        "close_price = data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_close_price = scaler.fit_transform(close_price)\n",
        "\n",
        "# Define the number of time steps\n",
        "n_steps = 10\n",
        "\n",
        "# Define a function to prepare data for LSTM\n",
        "def prepare_data(sequence, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "X, y = prepare_data(scaled_close_price, n_steps)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(n_steps, 1)))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_urauQCSrWxE",
        "outputId": "39eb77ce-183d-4eba-ff72-2900a02d5d41"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3675/3675 [==============================] - 26s 6ms/step - loss: 6.5746e-05\n",
            "Epoch 2/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 2.0495e-05\n",
            "Epoch 3/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.8402e-05\n",
            "Epoch 4/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.6718e-05\n",
            "Epoch 5/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.5099e-05\n",
            "Epoch 6/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.5524e-05\n",
            "Epoch 7/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.5751e-05\n",
            "Epoch 8/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.5378e-05\n",
            "Epoch 9/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.4997e-05\n",
            "Epoch 10/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.4470e-05\n",
            "Epoch 11/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.4702e-05\n",
            "Epoch 12/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.4456e-05\n",
            "Epoch 13/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.4397e-05\n",
            "Epoch 14/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.4333e-05\n",
            "Epoch 15/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.4068e-05\n",
            "Epoch 16/100\n",
            "3675/3675 [==============================] - 24s 7ms/step - loss: 1.3919e-05\n",
            "Epoch 17/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.4091e-05\n",
            "Epoch 18/100\n",
            "3675/3675 [==============================] - 22s 6ms/step - loss: 1.4020e-05\n",
            "Epoch 19/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.4182e-05\n",
            "Epoch 20/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.3873e-05\n",
            "Epoch 21/100\n",
            "3675/3675 [==============================] - 23s 6ms/step - loss: 1.3939e-05\n",
            "Epoch 22/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3668e-05\n",
            "Epoch 23/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3877e-05\n",
            "Epoch 24/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3219e-05\n",
            "Epoch 25/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3939e-05\n",
            "Epoch 26/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3591e-05\n",
            "Epoch 27/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3778e-05\n",
            "Epoch 28/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3496e-05\n",
            "Epoch 29/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3588e-05\n",
            "Epoch 30/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3846e-05\n",
            "Epoch 31/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3569e-05\n",
            "Epoch 32/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3721e-05\n",
            "Epoch 33/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3660e-05\n",
            "Epoch 34/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3420e-05\n",
            "Epoch 35/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3518e-05\n",
            "Epoch 36/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3624e-05\n",
            "Epoch 37/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3476e-05\n",
            "Epoch 38/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3428e-05\n",
            "Epoch 39/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3566e-05\n",
            "Epoch 40/100\n",
            "3675/3675 [==============================] - 19s 5ms/step - loss: 1.3642e-05\n",
            "Epoch 41/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3532e-05\n",
            "Epoch 42/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3332e-05\n",
            "Epoch 43/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3389e-05\n",
            "Epoch 44/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3567e-05\n",
            "Epoch 45/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3386e-05\n",
            "Epoch 46/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3746e-05\n",
            "Epoch 47/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.3205e-05\n",
            "Epoch 48/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3352e-05\n",
            "Epoch 49/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.3460e-05\n",
            "Epoch 50/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3311e-05\n",
            "Epoch 51/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3018e-05\n",
            "Epoch 52/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3472e-05\n",
            "Epoch 53/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3156e-05\n",
            "Epoch 54/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.3059e-05\n",
            "Epoch 55/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.3131e-05\n",
            "Epoch 56/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3460e-05\n",
            "Epoch 57/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2893e-05\n",
            "Epoch 58/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3478e-05\n",
            "Epoch 59/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3311e-05\n",
            "Epoch 60/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3150e-05\n",
            "Epoch 61/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3138e-05\n",
            "Epoch 62/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.2993e-05\n",
            "Epoch 63/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3301e-05\n",
            "Epoch 64/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3085e-05\n",
            "Epoch 65/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3117e-05\n",
            "Epoch 66/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.2984e-05\n",
            "Epoch 67/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3259e-05\n",
            "Epoch 68/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2671e-05\n",
            "Epoch 69/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3100e-05\n",
            "Epoch 70/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3157e-05\n",
            "Epoch 71/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3107e-05\n",
            "Epoch 72/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3175e-05\n",
            "Epoch 73/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3070e-05\n",
            "Epoch 74/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3094e-05\n",
            "Epoch 75/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.3184e-05\n",
            "Epoch 76/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3073e-05\n",
            "Epoch 77/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3232e-05\n",
            "Epoch 78/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.2927e-05\n",
            "Epoch 79/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3145e-05\n",
            "Epoch 80/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2870e-05\n",
            "Epoch 81/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3088e-05\n",
            "Epoch 82/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.2853e-05\n",
            "Epoch 83/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3206e-05\n",
            "Epoch 84/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.2896e-05\n",
            "Epoch 85/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3048e-05\n",
            "Epoch 86/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2936e-05\n",
            "Epoch 87/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3106e-05\n",
            "Epoch 88/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3084e-05\n",
            "Epoch 89/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2956e-05\n",
            "Epoch 90/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.2934e-05\n",
            "Epoch 91/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2823e-05\n",
            "Epoch 92/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.2897e-05\n",
            "Epoch 93/100\n",
            "3675/3675 [==============================] - 21s 6ms/step - loss: 1.2925e-05\n",
            "Epoch 94/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2873e-05\n",
            "Epoch 95/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.2781e-05\n",
            "Epoch 96/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.3128e-05\n",
            "Epoch 97/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2870e-05\n",
            "Epoch 98/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3024e-05\n",
            "Epoch 99/100\n",
            "3675/3675 [==============================] - 20s 6ms/step - loss: 1.2936e-05\n",
            "Epoch 100/100\n",
            "3675/3675 [==============================] - 20s 5ms/step - loss: 1.3115e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cff780838e0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the last time step of the data\n",
        "last_sequence = scaled_close_price[-n_steps:]\n",
        "last_sequence = last_sequence.reshape((1, n_steps, 1))\n",
        "predicted_close_price = model.predict(last_sequence)\n",
        "\n",
        "# Print predicted_close_price\n",
        "print(\"Predicted Close Price Array:\", predicted_close_price)\n",
        "\n",
        "# Convert the prediction array to a scalar\n",
        "predicted_close_price_scalar = np.squeeze(predicted_close_price)\n",
        "\n",
        "# Inverse transform the predicted close price scalar\n",
        "predicted_close_price_scalar = scaler.inverse_transform([[predicted_close_price_scalar]])\n",
        "\n",
        "# Compute evaluation metrics\n",
        "y_true = close_price[-1]\n",
        "y_pred = predicted_close_price_scalar[-1]\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "print(\"Sample Prediction Details:\")\n",
        "print(\"True Close Price:\", y_true)\n",
        "print(\"Predicted Close Price:\", y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGFlMmHvrWpv",
        "outputId": "ef6ffc2d-d544-4079-bc30-d513212b4843"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 887ms/step\n",
            "Predicted Close Price Array: [[0.00509273]]\n",
            "Sample Prediction Details:\n",
            "True Close Price: [185.6]\n",
            "Predicted Close Price: [176.46028106]\n",
            "Mean Absolute Error (MAE): 9.139718937873852\n",
            "Root Mean Squared Error (RMSE): 9.139718937873852\n",
            "Mean Absolute Percentage Error (MAPE): 4.924417531182032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacked Lstm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"/content/nifty50-stock-market-data/NIFTY50_all.csv\")\n",
        "\n",
        "# Select only the 'Close' price column\n",
        "close_price = data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_close_price = scaler.fit_transform(close_price)\n",
        "\n",
        "# Define the number of time steps\n",
        "n_steps = 10\n",
        "\n",
        "# Define a function to prepare data for LSTM\n",
        "def prepare_data(sequence, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "X, y = prepare_data(scaled_close_price, n_steps)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Define the stacked LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(n_steps, 1)))\n",
        "model.add(LSTM(units=50, return_sequences=True))  # Stacked LSTM layer\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk5cdbxvrWna",
        "outputId": "bf07428d-d7ab-4582-8aed-c5f6dd1887bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1838/1838 [==============================] - 18s 8ms/step - loss: 9.8148e-05\n",
            "Epoch 2/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 3.6402e-05\n",
            "Epoch 3/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 2.0248e-05\n",
            "Epoch 4/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.7878e-05\n",
            "Epoch 5/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.8709e-05\n",
            "Epoch 6/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.6668e-05\n",
            "Epoch 7/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.6544e-05\n",
            "Epoch 8/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5872e-05\n",
            "Epoch 9/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.6562e-05\n",
            "Epoch 10/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.6204e-05\n",
            "Epoch 11/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.6504e-05\n",
            "Epoch 12/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5791e-05\n",
            "Epoch 13/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5324e-05\n",
            "Epoch 14/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5999e-05\n",
            "Epoch 15/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5409e-05\n",
            "Epoch 16/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5011e-05\n",
            "Epoch 17/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5508e-05\n",
            "Epoch 18/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4517e-05\n",
            "Epoch 19/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5049e-05\n",
            "Epoch 20/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5196e-05\n",
            "Epoch 21/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4297e-05\n",
            "Epoch 22/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5155e-05\n",
            "Epoch 23/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4770e-05\n",
            "Epoch 24/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4303e-05\n",
            "Epoch 25/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4470e-05\n",
            "Epoch 26/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4933e-05\n",
            "Epoch 27/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4453e-05\n",
            "Epoch 28/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4452e-05\n",
            "Epoch 29/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4407e-05\n",
            "Epoch 30/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4161e-05\n",
            "Epoch 31/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4454e-05\n",
            "Epoch 32/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4299e-05\n",
            "Epoch 33/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4256e-05\n",
            "Epoch 34/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4159e-05\n",
            "Epoch 35/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4799e-05\n",
            "Epoch 36/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3953e-05\n",
            "Epoch 37/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4672e-05\n",
            "Epoch 38/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3972e-05\n",
            "Epoch 39/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4310e-05\n",
            "Epoch 40/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3953e-05\n",
            "Epoch 41/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4095e-05\n",
            "Epoch 42/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3672e-05\n",
            "Epoch 43/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3979e-05\n",
            "Epoch 44/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3991e-05\n",
            "Epoch 45/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3603e-05\n",
            "Epoch 46/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.5232e-05\n",
            "Epoch 47/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3618e-05\n",
            "Epoch 48/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3887e-05\n",
            "Epoch 49/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4404e-05\n",
            "Epoch 50/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3709e-05\n",
            "Epoch 51/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3563e-05\n",
            "Epoch 52/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3769e-05\n",
            "Epoch 53/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3730e-05\n",
            "Epoch 54/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4026e-05\n",
            "Epoch 55/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3719e-05\n",
            "Epoch 56/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3558e-05\n",
            "Epoch 57/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3853e-05\n",
            "Epoch 58/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3580e-05\n",
            "Epoch 59/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3445e-05\n",
            "Epoch 60/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3800e-05\n",
            "Epoch 61/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3714e-05\n",
            "Epoch 62/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4145e-05\n",
            "Epoch 63/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3508e-05\n",
            "Epoch 64/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3598e-05\n",
            "Epoch 65/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3591e-05\n",
            "Epoch 66/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3569e-05\n",
            "Epoch 67/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3889e-05\n",
            "Epoch 68/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3319e-05\n",
            "Epoch 69/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3072e-05\n",
            "Epoch 70/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3950e-05\n",
            "Epoch 71/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3647e-05\n",
            "Epoch 72/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3709e-05\n",
            "Epoch 73/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3537e-05\n",
            "Epoch 74/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.4010e-05\n",
            "Epoch 75/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3271e-05\n",
            "Epoch 76/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3552e-05\n",
            "Epoch 77/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3801e-05\n",
            "Epoch 78/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3473e-05\n",
            "Epoch 79/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3771e-05\n",
            "Epoch 80/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3622e-05\n",
            "Epoch 81/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3749e-05\n",
            "Epoch 82/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3502e-05\n",
            "Epoch 83/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3555e-05\n",
            "Epoch 84/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3453e-05\n",
            "Epoch 85/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3626e-05\n",
            "Epoch 86/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3335e-05\n",
            "Epoch 87/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3114e-05\n",
            "Epoch 88/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3452e-05\n",
            "Epoch 89/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3546e-05\n",
            "Epoch 90/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3530e-05\n",
            "Epoch 91/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3606e-05\n",
            "Epoch 92/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3617e-05\n",
            "Epoch 93/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3095e-05\n",
            "Epoch 94/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3403e-05\n",
            "Epoch 95/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3387e-05\n",
            "Epoch 96/100\n",
            "1838/1838 [==============================] - 14s 8ms/step - loss: 1.3107e-05\n",
            "Epoch 97/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3317e-05\n",
            "Epoch 98/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3174e-05\n",
            "Epoch 99/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3370e-05\n",
            "Epoch 100/100\n",
            "1838/1838 [==============================] - 13s 7ms/step - loss: 1.3469e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cff603f91e0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the last time step of the data\n",
        "last_sequence = scaled_close_price[-n_steps:]\n",
        "last_sequence = last_sequence.reshape((1, n_steps, 1))\n",
        "predicted_close_price = model.predict(last_sequence)\n",
        "\n",
        "# Print predicted_close_price\n",
        "print(\"Predicted Close Price Array:\", predicted_close_price)\n",
        "\n",
        "# Convert the prediction array to a scalar\n",
        "predicted_close_price_scalar = np.squeeze(predicted_close_price)\n",
        "\n",
        "# Inverse transform the predicted close price scalar\n",
        "predicted_close_price_scalar = scaler.inverse_transform([[predicted_close_price_scalar]])\n",
        "\n",
        "# Compute evaluation metrics\n",
        "y_true = close_price[-1]\n",
        "y_pred = predicted_close_price_scalar[-1]\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "print(\"Sample Prediction Details:\")\n",
        "print(\"True Close Price:\", y_true)\n",
        "print(\"Predicted Close Price:\", y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRwo2D8arWkk",
        "outputId": "ae66b812-03c3-4e46-d2e4-39484ad1db03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 845ms/step\n",
            "Predicted Close Price Array: [[0.00568493]]\n",
            "Sample Prediction Details:\n",
            "True Close Price: [185.6]\n",
            "Predicted Close Price: [195.91589561]\n",
            "Mean Absolute Error (MAE): 10.31589561104775\n",
            "Root Mean Squared Error (RMSE): 10.31589561104775\n",
            "Mean Absolute Percentage Error (MAPE): 5.5581334111248655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#univariate Bidirectional LSTM\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"/content/nifty50-stock-market-data/NIFTY50_all.csv\")\n",
        "\n",
        "# Select only the 'Close' price column\n",
        "close_price = data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_close_price = scaler.fit_transform(close_price)\n",
        "\n",
        "# Define the number of time steps\n",
        "n_steps = 10\n",
        "\n",
        "# Define a function to prepare data for LSTM\n",
        "def prepare_data(sequence, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "X, y = prepare_data(scaled_close_price, n_steps)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Define the Bidirectional LSTM model\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=50, input_shape=(n_steps, 1))))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s20_ws9c1-2X",
        "outputId": "55f46d3b-4c5d-4672-8636-c8406fb86132"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1838/1838 [==============================] - 13s 5ms/step - loss: 9.2102e-05\n",
            "Epoch 2/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 2.9572e-05\n",
            "Epoch 3/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.9448e-05\n",
            "Epoch 4/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.6003e-05\n",
            "Epoch 5/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.4539e-05\n",
            "Epoch 6/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.4058e-05\n",
            "Epoch 7/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.3755e-05\n",
            "Epoch 8/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.3438e-05\n",
            "Epoch 9/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.3140e-05\n",
            "Epoch 10/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.3220e-05\n",
            "Epoch 11/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.3139e-05\n",
            "Epoch 12/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2958e-05\n",
            "Epoch 13/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.3037e-05\n",
            "Epoch 14/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2916e-05\n",
            "Epoch 15/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2799e-05\n",
            "Epoch 16/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2797e-05\n",
            "Epoch 17/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2790e-05\n",
            "Epoch 18/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2724e-05\n",
            "Epoch 19/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.3015e-05\n",
            "Epoch 20/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2624e-05\n",
            "Epoch 21/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2707e-05\n",
            "Epoch 22/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2767e-05\n",
            "Epoch 23/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2974e-05\n",
            "Epoch 24/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2743e-05\n",
            "Epoch 25/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2751e-05\n",
            "Epoch 26/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2577e-05\n",
            "Epoch 27/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2655e-05\n",
            "Epoch 28/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2715e-05\n",
            "Epoch 29/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2652e-05\n",
            "Epoch 30/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2648e-05\n",
            "Epoch 31/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2680e-05\n",
            "Epoch 32/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2743e-05\n",
            "Epoch 33/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2487e-05\n",
            "Epoch 34/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2620e-05\n",
            "Epoch 35/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2524e-05\n",
            "Epoch 36/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2543e-05\n",
            "Epoch 37/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2462e-05\n",
            "Epoch 38/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2522e-05\n",
            "Epoch 39/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2550e-05\n",
            "Epoch 40/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2582e-05\n",
            "Epoch 41/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2698e-05\n",
            "Epoch 42/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2624e-05\n",
            "Epoch 43/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2446e-05\n",
            "Epoch 44/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2523e-05\n",
            "Epoch 45/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2464e-05\n",
            "Epoch 46/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2520e-05\n",
            "Epoch 47/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2470e-05\n",
            "Epoch 48/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2506e-05\n",
            "Epoch 49/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2571e-05\n",
            "Epoch 50/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2552e-05\n",
            "Epoch 51/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2476e-05\n",
            "Epoch 52/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2446e-05\n",
            "Epoch 53/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2511e-05\n",
            "Epoch 54/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2403e-05\n",
            "Epoch 55/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2461e-05\n",
            "Epoch 56/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2447e-05\n",
            "Epoch 57/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2355e-05\n",
            "Epoch 58/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2484e-05\n",
            "Epoch 59/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2397e-05\n",
            "Epoch 60/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2373e-05\n",
            "Epoch 61/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2407e-05\n",
            "Epoch 62/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2346e-05\n",
            "Epoch 63/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2458e-05\n",
            "Epoch 64/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2388e-05\n",
            "Epoch 65/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2565e-05\n",
            "Epoch 66/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2328e-05\n",
            "Epoch 67/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2351e-05\n",
            "Epoch 68/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2324e-05\n",
            "Epoch 69/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2372e-05\n",
            "Epoch 70/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2465e-05\n",
            "Epoch 71/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2388e-05\n",
            "Epoch 72/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2494e-05\n",
            "Epoch 73/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2353e-05\n",
            "Epoch 74/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2485e-05\n",
            "Epoch 75/100\n",
            "1838/1838 [==============================] - 11s 6ms/step - loss: 1.2276e-05\n",
            "Epoch 76/100\n",
            "1838/1838 [==============================] - 9s 5ms/step - loss: 1.2469e-05\n",
            "Epoch 77/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2307e-05\n",
            "Epoch 78/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2411e-05\n",
            "Epoch 79/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2404e-05\n",
            "Epoch 80/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2419e-05\n",
            "Epoch 81/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2407e-05\n",
            "Epoch 82/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2340e-05\n",
            "Epoch 83/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2425e-05\n",
            "Epoch 84/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2161e-05\n",
            "Epoch 85/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2346e-05\n",
            "Epoch 86/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2415e-05\n",
            "Epoch 87/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2375e-05\n",
            "Epoch 88/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2344e-05\n",
            "Epoch 89/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2280e-05\n",
            "Epoch 90/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2325e-05\n",
            "Epoch 91/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2230e-05\n",
            "Epoch 92/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2333e-05\n",
            "Epoch 93/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2331e-05\n",
            "Epoch 94/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2444e-05\n",
            "Epoch 95/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2316e-05\n",
            "Epoch 96/100\n",
            "1838/1838 [==============================] - 11s 6ms/step - loss: 1.2368e-05\n",
            "Epoch 97/100\n",
            "1838/1838 [==============================] - 10s 5ms/step - loss: 1.2379e-05\n",
            "Epoch 98/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2355e-05\n",
            "Epoch 99/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2336e-05\n",
            "Epoch 100/100\n",
            "1838/1838 [==============================] - 10s 6ms/step - loss: 1.2301e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cfefc6139d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the last time step of the data\n",
        "last_sequence = scaled_close_price[-n_steps:]\n",
        "last_sequence = last_sequence.reshape((1, n_steps, 1))\n",
        "predicted_close_price = model.predict(last_sequence)\n",
        "\n",
        "# Print predicted_close_price\n",
        "print(\"Predicted Close Price Array:\", predicted_close_price)\n",
        "\n",
        "# Convert the prediction array to a scalar\n",
        "predicted_close_price_scalar = np.squeeze(predicted_close_price)\n",
        "\n",
        "# Inverse transform the predicted close price scalar\n",
        "predicted_close_price_scalar = scaler.inverse_transform([[predicted_close_price_scalar]])\n",
        "\n",
        "# Compute evaluation metrics\n",
        "y_true = close_price[-1]\n",
        "y_pred = predicted_close_price_scalar[-1]\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "print(\"Sample Prediction Details:\")\n",
        "print(\"True Close Price:\", y_true)\n",
        "print(\"Predicted Close Price:\", y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH99VGtY1-z3",
        "outputId": "e6450ac7-1968-4a24-f8af-61e760dd6f95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 570ms/step\n",
            "Predicted Close Price Array: [[0.00510431]]\n",
            "Sample Prediction Details:\n",
            "True Close Price: [185.6]\n",
            "Predicted Close Price: [176.8408869]\n",
            "Mean Absolute Error (MAE): 8.759113100916153\n",
            "Root Mean Squared Error (RMSE): 8.759113100916153\n",
            "Mean Absolute Percentage Error (MAPE): 4.719349731097065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COdZXvCH4FxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}