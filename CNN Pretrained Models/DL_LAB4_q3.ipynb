{
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "DL_LAB4_q1",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2952603,
          "sourceType": "datasetVersion",
          "datasetId": 1794080
        }
      ],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install watermark"
      ],
      "metadata": {
        "id": "Gf9m2BXv_El7",
        "outputId": "c53bf951-9e5e-46ca-be11-f969ead6f9b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting watermark\n",
            "  Downloading watermark-2.4.3-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: ipython>=6.0 in /usr/local/lib/python3.10/dist-packages (from watermark) (7.34.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from watermark) (7.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from watermark) (67.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->watermark) (3.17.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.0->watermark)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0->watermark) (0.2.13)\n",
            "Installing collected packages: jedi, watermark\n",
            "Successfully installed jedi-0.19.1 watermark-2.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "8hrhVU-T_EdG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -a \"Rishit Kapoor 21BAI1046\" -u -d -v -m"
      ],
      "metadata": {
        "id": "jL7p1DTF_EMK",
        "outputId": "488cee0d-93a5-461d-92db-45f5d757b954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Rishit Kapoor 21BAI1046\n",
            "\n",
            "Last updated: 2024-02-24\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "Compiler    : GCC 11.4.0\n",
            "OS          : Linux\n",
            "Release     : 6.1.58+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'traffic-sign-dataset-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1794080%2F2952603%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240224%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240224T065357Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D02d29b6f0f954b25c509dfae204d56046aad49df6c2871101ce2c362cbf6957b12e55d268fc0843f7bf34a6c9076ed3afcf16b33863f7e852f1e526ebfaefa51fc0ed6951421cd62eff4d211890fa4be200191c851e1e31eac87bc9dbded4a5e5c0216c02a5b16a8aeb68d563190e0215899ecd051123e23e13569dd3a1e1042fe40faaf0bf57cb2ebc174924bd4637759527a34068cf9c2f96cdf1590a195fb570894948553ae3f1cd36b2db03ee6201015d8b096d0d0090b55baa48131709ed5f274c7eec2f05c4e892919871832a447a81b11c1d76bd6d5aa8e146f0170a1814e1ed7f18d09c80b5deb9c3494664870474e66984cba71b2fa1e17e44b95da'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "4mJ9xKXH5GWs",
        "outputId": "a294893a-b94b-4bb1-e867-4dc0deab0a55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading traffic-sign-dataset-classification, 199059936 bytes compressed\n",
            "[==================================================] 199059936 bytes downloaded\n",
            "Downloaded and uncompressed: traffic-sign-dataset-classification\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "traindata_gen = ImageDataGenerator(zoom_range=0.5, shear_range=0.8, horizontal_flip=True, rescale=1/255)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:15:38.551613Z",
          "iopub.execute_input": "2024-02-23T07:15:38.55203Z",
          "iopub.status.idle": "2024-02-23T07:15:38.557745Z",
          "shell.execute_reply.started": "2024-02-23T07:15:38.55199Z",
          "shell.execute_reply": "2024-02-23T07:15:38.556305Z"
        },
        "trusted": true,
        "id": "SjmVt7Pa5GWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/kaggle/input/traffic-sign-dataset-classification/traffic_Data/DATA'\n",
        "data_train_gen =traindata_gen.flow_from_directory(\n",
        "    directory=path,\n",
        "    target_size=(224,224),\n",
        "    batch_size=3,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode='rgb',\n",
        "    seed = 1234,\n",
        "    shuffle = True\n",
        "    )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:15:39.912308Z",
          "iopub.execute_input": "2024-02-23T07:15:39.912786Z",
          "iopub.status.idle": "2024-02-23T07:15:40.624251Z",
          "shell.execute_reply.started": "2024-02-23T07:15:39.91275Z",
          "shell.execute_reply": "2024-02-23T07:15:40.623362Z"
        },
        "trusted": true,
        "id": "rkt3adW85GWw",
        "outputId": "e416558f-373c-4227-b93d-7245ff37e862"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Found 4170 images belonging to 58 classes.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_images = []\n",
        "for img_path in glob.glob(\"/kaggle/input/traffic-sign-dataset-classification/traffic_Data/TEST/*\"):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize((224, 224))\n",
        "    img_array = np.array(img)\n",
        "    img_array = img_array / 255.0  # divide by 255.0 to get float values between 0 and 1 (Rescale)\n",
        "    all_images.append(img_array)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:15:50.650009Z",
          "iopub.execute_input": "2024-02-23T07:15:50.650463Z",
          "iopub.status.idle": "2024-02-23T07:16:16.630003Z",
          "shell.execute_reply.started": "2024-02-23T07:15:50.650425Z",
          "shell.execute_reply": "2024-02-23T07:16:16.629137Z"
        },
        "trusted": true,
        "id": "a1OGIEpI5GWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip '/content/traffic-sign-dataset-classification.zip'"
      ],
      "metadata": {
        "id": "5ukleYK18EEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "NJm0BDji5GWx",
        "outputId": "1f1a6d75-d4ca-4caf-95d8-208a3f42410d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "XliH9xuW7RZS",
        "outputId": "4d5450b8-41d9-4810-eb42-1e7163e7719d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "wat2ivAk7XKA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "GX0v3WzQ7Yfs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download ahemateja19bec1025/traffic-sign-dataset-classification"
      ],
      "metadata": {
        "id": "2r3_oT2y7dnk",
        "outputId": "6985cd97-bc67-41b7-d204-4c7e3eb368e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading traffic-sign-dataset-classification.zip to /content\n",
            " 99% 187M/190M [00:01<00:00, 175MB/s]\n",
            "100% 190M/190M [00:01<00:00, 161MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Set your path\n",
        "path = '/content/augmented'\n",
        "\n",
        "# Create ImageDataGenerator for augmentation\n",
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create a generator for the original dataset\n",
        "data_train_gen = data_gen.flow_from_directory(\n",
        "    directory=path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=3,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode='rgb',\n",
        "    seed=1234,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Augment a few images and save them to a new directory\n",
        "augmented_path = '/content/augmented'\n",
        "os.makedirs(augmented_path, exist_ok=True)\n",
        "\n",
        "num_augmented_images = 3\n",
        "\n",
        "for i in range(num_augmented_images):\n",
        "    batch = data_train_gen.next()\n",
        "    images = batch[0]\n",
        "\n",
        "    for j in range(len(images)):\n",
        "        augmented_image = images[j] * 255  # Assuming images are normalized\n",
        "        augmented_image = augmented_image.astype(np.uint8)\n",
        "        Image.fromarray(augmented_image).save(os.path.join(augmented_path, f\"augmented_{i}_{j}.png\"))\n",
        "\n",
        "# Merge the augmented images with the original dataset\n",
        "merged_path = '/content/augmented'\n",
        "os.makedirs(merged_path, exist_ok=True)\n",
        "\n",
        "original_images = os.listdir(path)\n",
        "augmented_images = os.listdir(augmented_path)\n",
        "\n",
        "for image in original_images + augmented_images:\n",
        "    image_path = os.path.join(path, image)\n",
        "    new_image_path = os.path.join(merged_path, image)\n",
        "    os.rename(image_path, new_image_path)\n",
        "\n",
        "# Display statistics before and after augmentation\n",
        "original_count = len(original_images)\n",
        "augmented_count = len(augmented_images)\n",
        "\n",
        "print(f\"Original dataset size: {original_count} images\")\n",
        "print(f\"Augmented dataset size: {augmented_count} images\")\n",
        "print(f\"Merged dataset size: {original_count + augmented_count} images\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-24T06:53:12.97762Z",
          "iopub.execute_input": "2024-02-24T06:53:12.978225Z"
        },
        "trusted": true,
        "id": "MioVWXqj5GWx",
        "outputId": "71537438-e5eb-478a-f715-16072e282d65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4170 images belonging to 58 classes.\n",
            "Original dataset size: 67 images\n",
            "Augmented dataset size: 67 images\n",
            "Merged dataset size: 134 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6duyqQ4g-pGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6cK2Zja70na"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}